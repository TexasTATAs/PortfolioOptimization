{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Portfolio_Optimisation_GPU.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tatsath/PortfolioOptimization/blob/main/Portfolio_Optimisation_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng9m5KlRNDQf",
        "outputId": "742f5dd1-be5c-4996-f006-73d242c130aa"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import numpy.matlib\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import norm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import time\n",
        "\n",
        "import copy\n",
        "\n",
        "#Other packages: \n",
        "import pandas as pd\n",
        "\n",
        "import pkg_resources\n",
        "import pip\n",
        "installedPackages = {pkg.key for pkg in pkg_resources.working_set}\n",
        "required = {'yfinance'}\n",
        "missing = required - installedPackages\n",
        "if missing:\n",
        "    !pip install yfinance   \n",
        "import yfinance as yf\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yfinance\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/e8/b9d7104d3a4bf39924799067592d9e59119fcfc900a425a12e80a3123ec8/yfinance-0.1.55.tar.gz\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.1.4)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/78/56a7c88a57d0d14945472535d0df9fb4bbad7d34ede658ec7961635c790e/lxml-4.6.2-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.55-py2.py3-none-any.whl size=22618 sha256=e1ece2219417b5552dd46219302e5aa313f1b599ade6095e0b1c9258d1ce2591\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/98/cc/2702a4242d60bdc14f48b4557c427ded1fe92aedf257d4565c\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.2 yfinance-0.1.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61D4LJ8fNDQf"
      },
      "source": [
        "# Dow 30 constituents at 2019/01/01\n",
        "dow_30_ticker = ['AAPL','MSFT','JPM','V','RTX','PG','GS','NKE','DIS','AXP',\n",
        "                  'HD','INTC','WMT','IBM','MRK','UNH','KO','CAT','TRV','JNJ',\n",
        "                  'CVX','MCD','VZ','CSCO','XOM','BA','MMM','PFE','WBA','DD', '^DJI'] "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOFh9LneNDQf",
        "outputId": "ebd4e1ec-63be-4dfa-d9b0-73a11fef7c94"
      },
      "source": [
        "dow_30 = pd.DataFrame()\n",
        "for tic in dow_30_ticker:\n",
        "    data_df = yf.download(tic, start=\"2009-01-01\", end=\"2020-10-23\")\n",
        "    data_df['tic'] = tic\n",
        "    dow_30=dow_30.append(data_df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ten2pxDmNDQg"
      },
      "source": [
        "dow_30.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE50wDujNDQg"
      },
      "source": [
        "Next Steps: \n",
        "#Pivot by the ticker and get the return and std \n",
        "#Get the meu sigma and std and correlation matrix of the stocks.\n",
        "https://towardsdatascience.com/simulating-stock-prices-in-python-using-geometric-brownian-motion-8dfd6e8c6b18\n",
        "#Get the weight as of 1st of November and see the weights of the benchmark intitially\n",
        "#What we will see is that more number of assets the weightes become more weird and then there is more deviation from the benchmark at T\n",
        "#Calibrate the model till 2017 and backtest on the next 3 years. \n",
        "\n",
        "Other analysis : \n",
        "\n",
        "* #Bond and equities benchmark.\n",
        "* #Why the model increases the weights all of a sudden on 15th day as compared to the benchmark ( to focus on the risk measure loss funcion)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULj7aKuuNDQh"
      },
      "source": [
        "# params = {\n",
        "#     \"S0\" : np.array([1]),\n",
        "#     \"mu\" : np.array([0.03]),\n",
        "#     \"sigma\" : np.array([0.2]),\n",
        "#     \"rho\" : np.array([[1]]),\n",
        "#     \"rf\" : 0.01,\n",
        "#     \"delta\" : np.array([0.7])\n",
        "#     }\n",
        "\n",
        "params = {\n",
        "    \"S0\" : np.array([1, 2]),\n",
        "    \"mu\" : np.array([0.05, 0.06]),\n",
        "    \"sigma\" : np.array([0.1, 0.12]),\n",
        "    \"rho\" : np.array([[1, 0.25],[0.25,1]]),\n",
        "    \"rf\" : 0.01,\n",
        "    \"delta\" : np.array([0.25, 0.75])\n",
        "    }\n",
        "\n",
        "params_Q = copy.copy(params)\n",
        "params_Q[\"mu\"] = np.array([params_Q[\"rf\"], params_Q[\"rf\"]])\n",
        "\n",
        "print(params)\n",
        "\n",
        "print(params_Q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKyKlyP_NDQh"
      },
      "source": [
        "#%% Simulation Engine for price paths\n",
        "def SimPath(Ndt, params, T, Nsims):\n",
        "    \n",
        "    dt = T/Ndt\n",
        "    \n",
        "    N_assets= params[\"mu\"].shape[0]\n",
        "    \n",
        "    assert N_assets == params[\"sigma\"].shape[0], \"#assets in mu != #assets in sigma\"\n",
        "    assert N_assets == params[\"rho\"].shape[0], \"#assets in mu != #assets in rho\"\n",
        "    assert params[\"rho\"].ndim == 2, \"rho must be matrix\"\n",
        "    assert N_assets == params[\"rho\"].shape[1], \"rho not square matrix\"\n",
        "    \n",
        "    S = np.zeros((Ndt+1, Nsims, N_assets ))\n",
        "    S[0,:,:] = params[\"S0\"]\n",
        "\n",
        "    \n",
        "    for i in range(Ndt):\n",
        "        \n",
        "        dW = np.sqrt(dt) * np.random.multivariate_normal(np.zeros(N_assets), params[\"rho\"], Nsims)\n",
        "        S[i+1,:,:] = S[i,:,:] * np.exp((params[\"mu\"]-0.5*params[\"sigma\"]**2)*dt + params[\"sigma\"]*dW)\n",
        "        \n",
        "    t = np.linspace(0,T,Ndt+1)\n",
        "    return t, S\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vinKioaYNDQh"
      },
      "source": [
        "# Sim some paths and show sims with quantiles and a sample path\n",
        "t, S = SimPath(Ndt=252, params = params, T=5, Nsims = 1000)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "plt.rc('axes', labelsize=22)\n",
        "for i in range(S.shape[2]):\n",
        "    \n",
        "    plt.subplot(1,S.shape[2],i+1)\n",
        "    \n",
        "    plt.fill_between(t, np.quantile(S[:,:,i],0.1,axis=1).T, np.quantile(S[:,:,i],0.9,axis=1).T, color='y', alpha=0.5)\n",
        "    plt.plot(t, S[:,:100,i],linewidth=0.3)\n",
        "    plt.plot(t, S[:,0,i],color='r',linewidth=1.5)\n",
        "    plt.plot(t, np.quantile(S[:,:,i],[0.1, 0.5, 0.9],axis=1).T,color='k',linewidth=1, linestyle='--')\n",
        "    \n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"$S_t^\" +str(i+1)+\"$\")\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdkzB3ZPNDQh"
      },
      "source": [
        "# the ANN for policy iteration\n",
        "class MyNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, n, n_assets ):\n",
        "        super(MyNet, self).__init__()\n",
        "\n",
        "        # 2 input layer (t, X_t^delta), 1 output channel, 3 hidden layers with n units each\n",
        "        self.f_in_to_h1 = nn.Linear( 2 , n)\n",
        "        self.f_h1_to_h2 = nn.Linear(n, n)\n",
        "        self.f_h2_to_out = nn.Linear(n, n_assets)\n",
        "        self.myReLU = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # input into 1st hidden layer\n",
        "        h1 = self.myReLU(self.f_in_to_h1(x) )\n",
        "        \n",
        "        # 1st hidden to 2nd hidden layer\n",
        "        h2 = self.myReLU(self.f_h1_to_h2(h1))\n",
        "            \n",
        "        # 2nd hidden layer to output layer\n",
        "        y = self.f_h2_to_out(h2)      \n",
        "        #y = y.clamp(-1,1)\n",
        "        #y = F.logsigmoid(h2)\n",
        "        \n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpOuz0duNDQh"
      },
      "source": [
        "# from a minibatch compute the risk measure\n",
        "def GetRiskMeasure(X, Type):\n",
        "    #TVaR    \n",
        "    if (Type == \"TVaR\"):\n",
        "        \n",
        "        alpha = 0.15\n",
        "        Qtl= torch.quantile(X, alpha)\n",
        "        RiskMeasure = -torch.mean(X[X<=Qtl])\n",
        "    \n",
        "    # alpha-beta\n",
        "    elif (Type == \"alpha-beta\" ):\n",
        "        \n",
        "        alpha = 0.1\n",
        "        LQtl = torch.quantile(X,alpha)\n",
        "        \n",
        "        beta = 0.9\n",
        "        UQtl = torch.quantile(X,beta)\n",
        "        \n",
        "        # weight factor (p in the paper)\n",
        "        p = 0.25\n",
        "        eta = p * alpha + (1-p) * (1-beta)\n",
        "        RiskMeasure = -(p* torch.mean(X[X<=LQtl]) \\\n",
        "                        + (1-p) * torch.mean(X[X>=UQtl] ) )/eta\n",
        "        \n",
        "        \n",
        "    #Variance\n",
        "    elif (Type == \"Variance\"):  \n",
        "        RiskMeasure = torch.var(X)   \n",
        "        \n",
        "    return RiskMeasure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF-vQFe1NDQh"
      },
      "source": [
        "# from a minibatch compute the Wasserstein Distance\n",
        "def getWassDistance(X, Y):\n",
        "    \n",
        "    # generate co-monotonic versions of the sample\n",
        "    X_sorted, _ = torch.sort(X)\n",
        "    Y_sorted, _ = torch.sort(Y)\n",
        "    \n",
        "    wass_dist = torch.sqrt(torch.mean(( X_sorted - Y_sorted )**2))\n",
        "    \n",
        "    return wass_dist "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP9RK7WvNDQh"
      },
      "source": [
        "# given an ANN run a simulation and compute the riskmeasure\n",
        "def SimRM(net, Ndt, T, Nsims, params, device):\n",
        "      \n",
        "    t, S = SimPath(Ndt, params, T=T, Nsims=Nsims)  \n",
        "    \n",
        "    # initial investment amount\n",
        "    X0 = 1\n",
        "    \n",
        "    num_asset = S.shape[2]\n",
        "    dt = t[1]-t[0]\n",
        "    \n",
        "    # don't think we need sensitivities w.r.t. this as they are fixed per simulation\n",
        "    S_t = torch.tensor(S, dtype=torch.float, requires_grad=False, device=device)\n",
        "    \n",
        "    S_Ret = (S_t[1:,:,:]-S_t[:-1,:,:])/S_t[:-1,:,:]\n",
        "    \n",
        "    #Benchmark Delta and return\n",
        "    Delta_t = torch.zeros((Ndt, Nsims, num_asset), \\\n",
        "                                 dtype=torch.float, requires_grad=False, device=device)\n",
        "    # note only 90% invested in risky assets here... 10% in risk-free\n",
        "    delta = torch.tensor(params[\"delta\"],requires_grad=False, device=device)\n",
        "    delta_r = 1 - torch.sum(delta)\n",
        "    Delta_t += delta \n",
        "\n",
        "    # accumulate returns as we go along\n",
        "    Benchmark_Ret = delta_r * params[\"rf\"] * dt+ torch.sum(Delta_t * S_Ret , axis=2)\n",
        "    \n",
        "    BenchmarkVal_path = torch.zeros((Ndt+1,Nsims), dtype=torch.float, requires_grad=False, device=device)\n",
        "    BenchmarkVal_path[0,:]  = X0\n",
        "    BenchmarkVal_path[1:,:] = X0*torch.cumprod( (1.0+Benchmark_Ret), axis=0)\n",
        "    BenchmarkVal_T = BenchmarkVal_path[-1,:].reshape(-1,1)\n",
        "    \n",
        "    #\n",
        "    # Getting the portfolio weights using benchmark value and time as features\n",
        "    #\n",
        "    x=np.zeros((Ndt+1, Nsims, 2))   \n",
        "    \n",
        "    x[:,:,0] = np.matlib.repmat(t.reshape(-1,1)/t[-1], 1, Nsims)\n",
        "    xt = torch.tensor(x, dtype=torch.float, device=device)\n",
        "    \n",
        "    xt[:,:,1] = BenchmarkVal_path/X0\n",
        "    \n",
        "    wt = net(xt)    \n",
        "    \n",
        "    # the candidate portolio\n",
        "    Portfolio_Ret = (1-torch.sum(wt[:-1,:,:], axis=2)) * params[\"rf\"] * dt+ torch.sum(wt[:-1,:,:] * S_Ret , axis=2)\n",
        "    \n",
        "    PortfolioVal_path = torch.zeros((Ndt+1,Nsims), dtype=torch.float, requires_grad=False, device=device)    \n",
        "    PortfolioVal_path[0,:] = X0\n",
        "    PortfolioVal_path[1:,:] = X0*torch.cumprod( (1.0+Portfolio_Ret), axis=0)\n",
        "    PortfolioVal_T = PortfolioVal_path[-1,:].reshape(-1,1)\n",
        "        \n",
        "    \n",
        "    # Compute the risk measure and wasstertein Distance for the computation\n",
        "    wass_dist = getWassDistance(PortfolioVal_T, BenchmarkVal_T)\n",
        "    RiskMeasure = GetRiskMeasure(PortfolioVal_T, \"alpha-beta\") \n",
        "    \n",
        "    return wt, RiskMeasure, wass_dist, PortfolioVal_T, BenchmarkVal_T, t, BenchmarkVal_path.cpu().detach().numpy(), PortfolioVal_path.cpu().detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSvz9xNtNDQh"
      },
      "source": [
        "# plot histogram of portfolio, sample path, and show dependence of X^\\pi on X^\\delta\n",
        "def PlotHists(X, Y, t, X_path, Y_path):\n",
        "    \n",
        "    Xcp = X.clone().cpu().detach().numpy()\n",
        "    Ycp = Y.clone().cpu().detach().numpy()\n",
        "    \n",
        "    plt.figure(figsize=(15,5))\n",
        "    \n",
        "    plt.subplot(1,3,1)\n",
        "    bins = np.linspace(0,4,50)\n",
        "    \n",
        "    sns.distplot(Xcp, hist=True, kde=True,  bins=bins, label='Benchmark')\n",
        "    sns.distplot(Ycp,  hist=True, kde=True, bins=bins, label='Portfolio')  \n",
        "    \n",
        "    plt.axvline(np.quantile(Xcp, 0.05),color='k',linewidth=0.5)\n",
        "    plt.axvline(np.quantile(Ycp, 0.05),color='r',linewidth=0.5)\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.xlim(0,4)\n",
        "    \n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.fill_between(t, np.quantile(X_path,0.1,axis=1).T, np.quantile(X_path,0.9,axis=1).T, color='y', alpha=0.5)\n",
        "    plt.fill_between(t, np.quantile(Y_path,0.1,axis=1).T, np.quantile(Y_path,0.9,axis=1).T, color='g', alpha=0.5)\n",
        "    plt.plot(t,X_path[:,0], label='Benchmark', color='r')\n",
        "    plt.plot(t,np.quantile(X_path,[0.1,0.5,0.9],axis=1).T, '--y')\n",
        "    plt.plot(t,Y_path[:,0], label='Portfolio', color='k')\n",
        "    plt.plot(t,np.quantile(Y_path,[0.1,0.5,0.9],axis=1).T, '--g')\n",
        "    \n",
        "    plt.legend()\n",
        "    \n",
        "    \n",
        "    plt.subplot(1,3,3)\n",
        "    xs = [min(Xcp), max(Xcp)]\n",
        "    plt.plot(xs,xs,'--k' )\n",
        "    plt.scatter(Xcp,Ycp, marker='o',color='r', s=5)\n",
        "    plt.xlim(0,5)\n",
        "    plt.ylim(0,5)\n",
        "    \n",
        "    plt.rcParams.update({'font.size': 16})\n",
        "    plt.rc('axes', labelsize=22)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_lHBHUbNDQh"
      },
      "source": [
        "# create an ANN and use GPU if available\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda') \n",
        "else:\n",
        "    device = torch.device('cpu') \n",
        "\n",
        "net = MyNet(50, len(params[\"S0\"])).to(device)\n",
        "\n",
        "wt, RiskMeasure, wass_dist, PortfolioVal_T, BenchmarkVal_T, \\\n",
        "    t, BenchmarkVal_path, PortfolioVal_path = SimRM(net, Ndt=500, T=5, Nsims=1000, params=params, device=device)\n",
        "    \n",
        "PlotHists(BenchmarkVal_T, PortfolioVal_T, t, BenchmarkVal_path, PortfolioVal_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHedCN5LNDQh"
      },
      "source": [
        "PortfolioVal_path.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcmdl3KYNDQh"
      },
      "source": [
        "# train the model\n",
        "\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "\n",
        "Nepochs = 100\n",
        "loss_hist = []\n",
        "risk_history = []\n",
        "wass_dist_history=[]\n",
        "\n",
        "start_time= time.time()\n",
        "\n",
        "for epoch in range(Nepochs):  # loop over the dataset multiple times\n",
        "\n",
        "    \n",
        "    \n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    #hedge_payoff, true_payoff, S = SimHedge(net, Ndt, S0, mu, sigma, T, Nsims, True)\n",
        "    wt, RiskMeasure, wass_dist, PortfolioVal_T, IndexVal_T, t, IndexVal_path, PortfolioVal_path \\\n",
        "        = SimRM(net, Ndt=500, T=5, Nsims=10000, params=params, device=device)\n",
        "    \n",
        "    loss = 100*(wass_dist)*(wass_dist>0.2) + RiskMeasure\n",
        "    \n",
        "    \n",
        "    loss.backward()   \n",
        "    \n",
        "    # optimize\n",
        "    optimizer.step()\n",
        "    \n",
        "    # store running loss\n",
        "    loss_hist.append(  loss.item() )\n",
        "    risk_history.append( RiskMeasure.cpu().detach().numpy() )\n",
        "    wass_dist_history.append( wass_dist.cpu().detach().numpy() )\n",
        "    \n",
        "    print(\".\",end=\"\")\n",
        "    \n",
        "    # plot output every 50 iterations\n",
        "    if( (epoch % 10 == 0) and (epoch>1) ):\n",
        "        \n",
        "        print(epoch, end=\" \")\n",
        "        print(\"duaration = \", \"{:.3f}\".format(time.time() - start_time), \" secs\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        print(\"Wass Dist:\", \"{:.3f}\".format(wass_dist_history[-1]), end=\" \" )\n",
        "        print(\"Risk Measure:\", \"{:.3f}\".format(risk_history[-1]), end=\" \")\n",
        "        print(\"Loss:\", \"{:.3f}\".format(loss.item()))\n",
        "\n",
        "        PlotHists(IndexVal_T, PortfolioVal_T, t, IndexVal_path, PortfolioVal_path)\n",
        "                \n",
        "        \n",
        "        _, _, _, PortfolioVal_T_Q, BenchmarkVal_T_Q, _, _, _ = SimRM(net, Ndt=500, T=5, Nsims=10000, params=params_Q, device=device)\n",
        "        print(np.mean(np.exp(-params_Q[\"rf\"]*5) * BenchmarkVal_T_Q.cpu().detach().numpy()), end=\" \")\n",
        "        print(np.mean(np.exp(-params_Q[\"rf\"]*5) * PortfolioVal_T_Q.cpu().detach().numpy()), end=\"\\n\\n\")        \n",
        "        \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsA7nOCnNDQh"
      },
      "source": [
        "X_pi_T = np.zeros((0,1))\n",
        "X_delta_T = np.zeros((0,1))\n",
        "for i in range(10):\n",
        "    _, _, _, PortfolioVal_T_Q, BenchmarkVal_T_Q, _, _, _ = SimRM(net, Ndt=500, T=5, Nsims=10000, params=params_Q, device=device)\n",
        "    X_pi_T = np.concatenate((X_pi_T,PortfolioVal_T_Q.cpu().detach().numpy()))\n",
        "    X_delta_T = np.concatenate((X_pi_T,BenchmarkVal_T_Q.cpu().detach().numpy()))\n",
        "    \n",
        "print(np.mean(np.exp(-params_Q[\"rf\"]*5) * X_delta_T), end=\" \")\n",
        "print(np.mean(np.exp(-params_Q[\"rf\"]*5) * X_pi_T), end=\"\\n\\n\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwHcn88TNDQh"
      },
      "source": [
        "X_delta_T.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iASGCiQhNDQh"
      },
      "source": [
        "def PlotStrat(net, device):\n",
        "    \n",
        "    t = np.linspace(0, 5, 100)\n",
        "    X_delta_t = np.linspace(0.5, 3, 100)\n",
        "    \n",
        "    x1, x2 = np.meshgrid(t, X_delta_t)\n",
        "    \n",
        "    x = np.zeros((x1.shape[0], x1.shape[1], 2))\n",
        "    \n",
        "    x[:,:,0] = x1\n",
        "    x[:,:,1] = x2\n",
        "    \n",
        "    x_t = torch.tensor(x, dtype=torch.float, device=device )\n",
        "    \n",
        "    w_t = net(x_t).cpu().detach().numpy()\n",
        "    \n",
        "    \n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    \n",
        "    for i in range(w_t.shape[2]):\n",
        "    \n",
        "        \n",
        "        ax = fig.add_subplot(1, w_t.shape[2], i+1, projection='3d')\n",
        "        \n",
        "        ax.plot_surface(x1, x2, w_t[:,:,i], cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
        "        \n",
        "        ax.view_init(30, 30)\n",
        "        \n",
        "        plt.title('Asset' + str(i+1))\n",
        "        \n",
        "        plt.xlabel('t')\n",
        "        plt.ylabel('x')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    for i in range(0,x1.shape[1],5):\n",
        "    \n",
        "        fig = plt.figure(figsize=(10,5))\n",
        "    \n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(X_delta_t, w_t[:,i,0])\n",
        "        \n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(X_delta_t, w_t[:,i,1])\n",
        "        \n",
        "        plt.suptitle(\"t={0:0.2f}\".format(t[i]) , fontsize= 20 )\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "    \n",
        "    \n",
        "PlotStrat(net, device)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLgPv88tNDQh"
      },
      "source": [
        "%debug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0mi-od1NDQh"
      },
      "source": [
        "import pickle "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VSKxihiNDQh"
      },
      "source": [
        "pickle.dump( net, open( \"net_Nov2.p\", \"wb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV6ix4eeNDQh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}